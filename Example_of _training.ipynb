{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71eedbf5-aa6e-48b2-ab80-e27fdfa25e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "from typing import Dict, Callable, List, Tuple, Optional\n",
    "from einops import rearrange\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, cohen_kappa_score, balanced_accuracy_score,\n",
    "    confusion_matrix, log_loss\n",
    ")\n",
    "from ptflops import get_model_complexity_info\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3502edec-6674-49f7-8a05-33bb35bd40ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]\n",
      "Torch: 2.1.2+cu121\n",
      "Torchvision: 0.16.2+cu121\n",
      "NumPy: 1.26.3\n",
      "Pandas: 2.2.3\n",
      "scikit-learn: 1.6.1\n",
      "Pillow: 10.2.0\n",
      "Matplotlib: 3.8.2\n",
      "einops: 0.8.1\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import sklearn\n",
    "import PIL\n",
    "import matplotlib\n",
    "import einops\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"Pillow:\", PIL.__version__)\n",
    "print(\"Matplotlib:\", matplotlib.__version__)\n",
    "print(\"einops:\", einops.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5c6587-46e3-4375-92a7-8df3d226fa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1080, 224, 224, 3])\n",
      "torch.Size([120, 224, 224, 3])\n",
      "torch.Size([400, 224, 224, 3])\n",
      "Shape of the training set images: torch.Size([1080, 3, 224, 224])\n",
      "Shape of training set labels: torch.Size([1080])\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/root/autodl-fs/retinamnist_224.npz')\n",
    "train_images = data['train_images']\n",
    "train_labels = data['train_labels']\n",
    "val_images = data['val_images']\n",
    "val_labels = data['val_labels']\n",
    "test_images = data['test_images']\n",
    "test_labels = data['test_labels']\n",
    "data.close()\n",
    "\n",
    "def to_tensor(images, labels):\n",
    "    images = torch.from_numpy(images).float()\n",
    "    print(images.shape)\n",
    "    if images.dim() == 3:\n",
    "        images = images.unsqueeze(-1).repeat(1, 1, 1, 3)\n",
    "    images = images.permute(0, 3, 1, 2)\n",
    "    labels = torch.from_numpy(labels).long().squeeze()\n",
    "    return images, labels\n",
    "\n",
    "train_images, train_labels = to_tensor(train_images, train_labels)\n",
    "val_images, val_labels = to_tensor(val_images, val_labels)\n",
    "test_images, test_labels = to_tensor(test_images, test_labels)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = MNISTDataset(train_images, train_labels, transform=train_transform)\n",
    "test_dataset = MNISTDataset(test_images, test_labels, transform=test_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"Shape of the training set images:\", train_images.shape) \n",
    "print(\"Shape of training set labels:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "172c6ee4-e139-4d6e-9365-8b70e1dea4fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Missing] head.weight, head.bias...\n",
      "Reinitialized: head\n",
      "✅ Success: EMO-2M\n",
      "\n",
      "Loaded Models:\n",
      "EMO-2M          : Loaded\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 5\n",
    "BASE_PATH = Path(\"/root/models\")\n",
    "HEAD_LAYER_NAMES = (\"head\", \"classifier\", \"fc\", \"logits\")\n",
    "\n",
    "def dynamic_import(model_path: Path, module_path: str):\n",
    "    sys.path.insert(0, str(model_path))\n",
    "    try:\n",
    "        module = __import__(module_path)\n",
    "        return module\n",
    "    finally:\n",
    "        sys.path.pop(0)\n",
    "\n",
    "_ = dynamic_import(f\"{BASE_PATH}/EMO\", \"emo_models\")\n",
    "from emo_models import EMO_2M\n",
    "\n",
    "def _filter_head_keys(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    return {k: v for k, v in state_dict.items() if not any(layer_name in k for layer_name in HEAD_LAYER_NAMES)}\n",
    "\n",
    "def load_pretrained_weights(model: nn.Module, ckpt_path: Path) -> None:\n",
    "    checkpoint = torch.load(str(ckpt_path), map_location=\"cpu\")\n",
    "    state_dict = checkpoint.get(\"model\", checkpoint)\n",
    "    filtered = _filter_head_keys(state_dict)\n",
    "    result = model.load_state_dict(filtered, strict=False)\n",
    "    if result.missing_keys:\n",
    "        print(f\"[Missing] {', '.join(result.missing_keys[:3])}...\")\n",
    "    if result.unexpected_keys:\n",
    "        print(f\"[Extra] {', '.join(result.unexpected_keys[:3])}...\")\n",
    "\n",
    "def init_classification_head(model: nn.Module) -> None:\n",
    "    for name, module in model.named_modules():\n",
    "        if any(name.endswith(layer) for layer in HEAD_LAYER_NAMES):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, nonlinearity=\"relu\")\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "                print(f\"Reinitialized: {name}\")\n",
    "\n",
    "def load_simple_checkpoint(model_constructor: Callable[..., nn.Module], ckpt_path: Path) -> nn.Module:\n",
    "    model = model_constructor(num_classes=NUM_CLASSES)\n",
    "    load_pretrained_weights(model, ckpt_path)\n",
    "    init_classification_head(model)\n",
    "    return model\n",
    "\n",
    "def load_complex_checkpoint(model_constructor: Callable[..., nn.Module], ckpt_path: Path, key: str = \"model\", shape_transform: bool = False) -> nn.Module:\n",
    "    model = model_constructor(num_classes=NUM_CLASSES)\n",
    "    checkpoint = torch.load(str(ckpt_path), map_location=\"cpu\")\n",
    "    state_dict = checkpoint.get(key, checkpoint)\n",
    "    if shape_transform:\n",
    "        target = model.state_dict()\n",
    "        for k in list(state_dict.keys()):\n",
    "            if k in target and state_dict[k].ndim != target[k].ndim:\n",
    "                state_dict[k] = state_dict[k].view(*target[k].shape)\n",
    "    filtered = _filter_head_keys(state_dict)\n",
    "    model.load_state_dict(filtered, strict=False)\n",
    "    init_classification_head(model)\n",
    "    return model\n",
    "\n",
    "model_configs = [\n",
    "    (\"EMO-2M\",\n",
    "     lambda: load_simple_checkpoint(\n",
    "         EMO_2M,\n",
    "         f\"{BASE_PATH}/EMO/weights/net.pth\"\n",
    "     )),\n",
    "]\n",
    "\n",
    "def build_models(configs: list[tuple[str, Callable[[], nn.Module]]]) -> Dict[str, Optional[nn.Module]]:\n",
    "    built = {}\n",
    "    for name, builder in configs:\n",
    "        try:\n",
    "            built[name] = builder()\n",
    "            print(f\"✅ Success: {name}\")\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"❌ Failed to load {name}: {e}\")\n",
    "            built[name] = None\n",
    "    return built\n",
    "\n",
    "models = build_models(model_configs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nLoaded Models:\")\n",
    "    for name, model in models.items():\n",
    "        status = \"Loaded\" if model else \"Failed\"\n",
    "        print(f\"{name.ljust(16)}: {status}\")\n",
    "    sample = models.get(\"EMO_2M\")\n",
    "    if sample is not None:\n",
    "        print(\"\\nSample Model Architecture:\")\n",
    "        print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e899de4a-c701-4f98-a6d1-5095d5703a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softplus_floor(x, eps=1e-6):\n",
    "    return F.softplus(x) + eps\n",
    "\n",
    "def _collect_logits_labels(model: nn.Module, data_loader, device):\n",
    "    model.eval()\n",
    "    logits_list, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            logits_list.append(outputs)\n",
    "            labels_list.append(labels)\n",
    "    return torch.cat(logits_list), torch.cat(labels_list)\n",
    "\n",
    "class MiTLoss_WithTrainCalibration(nn.Module):\n",
    "    def __init__(self, num_classes: int, train_loader, model: nn.Module, device):\n",
    "        super().__init__()\n",
    "        self.num_classes = int(num_classes)\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "\n",
    "        # ---- Initialize temperature using training set ----\n",
    "        init_tau = self._initialize_temperature()\n",
    "        self.tau = nn.Parameter(init_tau)\n",
    "\n",
    "        # ---- Running class histogram (for empirical label entropy H*) ----\n",
    "        self.register_buffer(\"class_counts\", torch.ones(self.num_classes))\n",
    "        self.register_buffer(\"total_seen\", torch.tensor(self.num_classes, dtype=torch.long))\n",
    "\n",
    "        # ---- Dual-averaged λ ----\n",
    "        self.register_buffer(\"lambda_entropy\", torch.tensor(0.1))\n",
    "        self.register_buffer(\"dual_updates\", torch.tensor(0.1, dtype=torch.long))\n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    def _initialize_temperature(self):\n",
    "        warnings.warn(\"Initializing temperature using the training set.\")\n",
    "        logits, labels = _collect_logits_labels(self.model, self.train_loader, self.device)\n",
    "        logits, labels = logits.to(self.device), labels.to(self.device)\n",
    "\n",
    "        logT = torch.tensor(0.0, device=self.device, requires_grad=True)\n",
    "        opt = torch.optim.LBFGS([logT], lr=0.1, max_iter=50, line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            T = torch.exp(logT)\n",
    "            log_probs = F.log_softmax(logits / T, dim=1)\n",
    "            nll = F.nll_loss(log_probs, labels, reduction='mean')\n",
    "            nll.backward()\n",
    "            return nll\n",
    "\n",
    "        opt.step(closure)\n",
    "        T_star = torch.exp(logT).detach()               \n",
    "        tau0 = torch.log(torch.expm1(T_star).clamp_min(1e-12))\n",
    "        return tau0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _update_label_entropy(self, targets: torch.Tensor):\n",
    "        dev = self.class_counts.device\n",
    "        targets = targets.to(dev, dtype=torch.long)\n",
    "        binc = torch.bincount(targets, minlength=self.num_classes).to(dev, dtype=self.class_counts.dtype)\n",
    "        self.class_counts += binc\n",
    "        self.total_seen += targets.numel()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _empirical_label_entropy(self) -> torch.Tensor:\n",
    "        probs = self.class_counts / self.class_counts.sum()\n",
    "        logp = torch.log(probs.clamp_min(1e-12))\n",
    "        H_star = -(probs * logp).sum()\n",
    "        return H_star\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n",
    "        dev = logits.device\n",
    "        targets = targets.to(dev, dtype=torch.long)\n",
    "\n",
    "        # 1) Update H* from data\n",
    "        self._update_label_entropy(targets)\n",
    "        H_star = self._empirical_label_entropy()\n",
    "        H_max = math.log(self.num_classes + 1e-12)\n",
    "\n",
    "        # 2) Temperature scaling\n",
    "        T = _softplus_floor(self.tau).clamp(1e-3, 500.0)\n",
    "        scaled = logits / T\n",
    "\n",
    "        # 3) Cross-entropy loss\n",
    "        ce_loss = self.ce(scaled, targets)\n",
    "\n",
    "        # 4) Entropy of predictive distribution\n",
    "        log_probs = F.log_softmax(scaled, dim=1)\n",
    "        probs = log_probs.exp()\n",
    "        H = -(probs * log_probs).sum(dim=1).mean()\n",
    "\n",
    "        # 5) Update λ via dual-averaging\n",
    "        with torch.no_grad():\n",
    "            d = (H_star - H) / max(H_max, 1e-12)\n",
    "            d = torch.clamp(d, min=0.0)\n",
    "            self.dual_updates += 1\n",
    "            new_lambda = (self.lambda_entropy * (self.dual_updates - 1) + d) / self.dual_updates\n",
    "            new_lambda = torch.clamp(new_lambda, 0.0, 0.5)\n",
    "            self.lambda_entropy.copy_(new_lambda)\n",
    "\n",
    "        # 6) Combined loss\n",
    "        loss = ce_loss - self.lambda_entropy * H\n",
    "\n",
    "        stats = {\n",
    "            \"loss\": loss.detach(),\n",
    "            \"ce\": ce_loss.detach(),\n",
    "            \"H\": H.detach(),\n",
    "            \"H_star\": H_star.detach(),\n",
    "            \"lambda\": self.lambda_entropy.detach(),\n",
    "            \"T\": T.detach()\n",
    "        }\n",
    "        return loss, stats\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def calculate_specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    specificity = 0.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i]\n",
    "        denominator = tn + fp\n",
    "        specificity += tn / denominator if denominator != 0 else 0.0\n",
    "    return specificity / cm.shape[0]\n",
    "\n",
    "def train(epoch, net, optimizer, criterion, train_metrics):\n",
    "    print('\\nEpoch:', epoch)\n",
    "    net.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    targets_all, predicted_all = [], []\n",
    "    train_T_list, train_lambda_list = [], []\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss, stats = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        T_batch = float(stats[\"T\"].item())\n",
    "        lambda_batch = float(stats[\"lambda\"].item())\n",
    "        logits_scaled = outputs / T_batch\n",
    "        probs = torch.softmax(logits_scaled, dim=1)\n",
    "        _, predicted = probs.max(1)\n",
    "        train_loss += loss.item()\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        targets_all.extend(targets.detach().cpu().numpy())\n",
    "        predicted_all.extend(predicted.detach().cpu().numpy())\n",
    "        train_T_list.append(T_batch)\n",
    "        train_lambda_list.append(lambda_batch)\n",
    "    OA = 100 * accuracy_score(targets_all, predicted_all)\n",
    "    P = 100 * precision_score(targets_all, predicted_all, average='macro')\n",
    "    Se = 100 * recall_score(targets_all, predicted_all, average='macro')\n",
    "    Sp = 100 * calculate_specificity(targets_all, predicted_all)\n",
    "    F1 = 100 * f1_score(targets_all, predicted_all, average='macro')\n",
    "    Kappa = 100 * cohen_kappa_score(targets_all, predicted_all)\n",
    "    T_epoch = float(np.mean(train_T_list)) if train_T_list else float(stats[\"T\"].item())\n",
    "    lambda_epoch = float(np.mean(train_lambda_list)) if train_lambda_list else float(stats[\"lambda\"].item())\n",
    "    train_metrics['loss'].append(train_loss / len(train_loader))\n",
    "    train_metrics['P'].append(P)\n",
    "    train_metrics['Se'].append(Se)\n",
    "    train_metrics['Sp'].append(Sp)\n",
    "    train_metrics['F1'].append(F1)\n",
    "    train_metrics['OA'].append(OA)\n",
    "    train_metrics['Kappa'].append(Kappa)\n",
    "    train_metrics['T'].append(T_epoch)\n",
    "    train_metrics['lambda'].append(lambda_epoch)\n",
    "    print(f'Train Loss: {train_loss/len(train_loader):.3f} | OA: {OA:.1f}% | P: {P:.1f} | Se: {Se:.1f} | Sp: {Sp:.1f} | F1: {F1:.1f} | Kappa: {Kappa:.1f} | T(avg): {T_epoch:.3f} | lambda(avg): {lambda_epoch:.3f}')\n",
    "\n",
    "class ModelSaver:\n",
    "    def __init__(self, model_name):\n",
    "        self.best_metrics = {'OA': 0.0, 'AUC': 0.0, 'F1': 0.0, 'P': 0.0, 'Se': 0.0, 'Sp': 0.0, 'Kappa': 0.0}\n",
    "        self.save_dir = os.path.join('best_models', model_name)\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "    def check_and_save(self, net, criterion, current_metrics):\n",
    "        for metric in ['OA', 'AUC', 'F1', 'Kappa']:\n",
    "            if current_metrics[metric] > self.best_metrics[metric]:\n",
    "                self.best_metrics[metric] = current_metrics[metric]\n",
    "                torch.save({\"model\": net.state_dict(), \"criterion\": criterion.state_dict()}, os.path.join(self.save_dir, f'best_{metric}.pth'))\n",
    "        torch.save({\"model\": net.state_dict(), \"criterion\": criterion.state_dict()}, os.path.join(self.save_dir, 'final_model.pth'))\n",
    "\n",
    "def val(epoch, net, criterion, val_metrics, model_saver):\n",
    "    net.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    targets_all, predicted_all, probabilities_all = [], [], []\n",
    "    val_T_list = []\n",
    "    val_lambda_list = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss, stats = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            T_batch = float(stats[\"T\"].item())\n",
    "            lambda_batch = float(stats[\"lambda\"].item())\n",
    "            logits_scaled = outputs / T_batch\n",
    "            probs = torch.softmax(logits_scaled, dim=1)\n",
    "            _, predicted = probs.max(1)\n",
    "            probabilities_all.extend(probs.detach().cpu().numpy())\n",
    "            targets_all.extend(targets.detach().cpu().numpy())\n",
    "            predicted_all.extend(predicted.detach().cpu().numpy())\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            val_T_list.append(T_batch)\n",
    "            val_lambda_list.append(lambda_batch)\n",
    "    OA = 100 * accuracy_score(targets_all, predicted_all)\n",
    "    P = 100 * precision_score(targets_all, predicted_all, average='macro')\n",
    "    Se = 100 * recall_score(targets_all, predicted_all, average='macro')\n",
    "    Sp = 100 * calculate_specificity(targets_all, predicted_all)\n",
    "    F1 = 100 * f1_score(targets_all, predicted_all, average='macro')\n",
    "    Kappa = 100 * cohen_kappa_score(targets_all, predicted_all)\n",
    "    try:\n",
    "        n_classes = len(np.unique(targets_all))\n",
    "        if n_classes == 2:\n",
    "            AUC = 100 * roc_auc_score(targets_all, np.array(probabilities_all)[:, 1])\n",
    "        else:\n",
    "            AUC = 100 * roc_auc_score(targets_all, probabilities_all, multi_class='ovr', average='macro')\n",
    "    except Exception as e:\n",
    "        print(f\"AUC calculation failed: {str(e)}\")\n",
    "        AUC = 0.0\n",
    "    T_avg = float(np.mean(val_T_list)) if val_T_list else float(stats[\"T\"].item())\n",
    "    lambda_avg = float(np.mean(val_lambda_list)) if val_lambda_list else float(stats[\"lambda\"].item())\n",
    "    val_metrics['loss'].append(val_loss / len(test_loader))\n",
    "    val_metrics['P'].append(P)\n",
    "    val_metrics['Se'].append(Se)\n",
    "    val_metrics['Sp'].append(Sp)\n",
    "    val_metrics['F1'].append(F1)\n",
    "    val_metrics['OA'].append(OA)\n",
    "    val_metrics['AUC'].append(AUC)\n",
    "    val_metrics['Kappa'].append(Kappa)\n",
    "    val_metrics['T'].append(T_avg)\n",
    "    val_metrics['lambda'].append(lambda_avg)\n",
    "    current_metrics = {'OA': OA, 'AUC': AUC, 'F1': F1, 'P': P, 'Se': Se, 'Sp': Sp, 'Kappa': Kappa, 'T': T_avg, 'lambda': lambda_avg}\n",
    "    model_saver.check_and_save(net, criterion, current_metrics)\n",
    "    print(f'Val  Loss: {val_loss/len(test_loader):.3f} | OA: {OA:.1f}% | P: {P:.1f} | Se: {Se:.1f} | Sp: {Sp:.1f} | F1: {F1:.1f} | AUC: {AUC:.1f} | Kappa: {Kappa:.1f} | T(avg): {T_avg:.3f} | lambda(avg): {lambda_avg:.3f}')\n",
    "\n",
    "def train_and_save_model(model, model_name, train_loader, test_loader, num_classes, num_epochs=10):\n",
    "    train_metrics = {key: [] for key in ['loss', 'P', 'Se', 'Sp', 'F1', 'OA', 'Kappa', 'T', 'lambda']}\n",
    "    val_metrics = {key: [] for key in ['loss', 'P', 'Se', 'Sp', 'F1', 'OA', 'AUC', 'Kappa', 'T', 'lambda']}\n",
    "    net = model.to(device)\n",
    "    criterion = MiTLoss_WithTrainCalibration(num_classes, train_loader, model, device)\n",
    "    optimizer = torch.optim.Adam(list(net.parameters()) + list(criterion.parameters()), lr=0.0001)\n",
    "    model_saver = ModelSaver(model_name)\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train(epoch, net, optimizer, criterion, train_metrics)\n",
    "        val(epoch, net, criterion, val_metrics, model_saver)\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} Time: {epoch_time:.1f}s')\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "    save_metrics_to_csv(train_metrics, os.path.join(model_name, 'train_metrics.csv'))\n",
    "    save_metrics_to_csv(val_metrics, os.path.join(model_name, 'val_metrics.csv'))\n",
    "\n",
    "def save_metrics_to_csv(metrics, file_path):\n",
    "    fieldnames = ['epoch'] + list(metrics.keys())\n",
    "    epochs = range(1, len(next(iter(metrics.values()))) + 1)\n",
    "    with open(file_path, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for epoch in epochs:\n",
    "            row = {'epoch': epoch}\n",
    "            for key in metrics.keys():\n",
    "                row[key] = metrics[key][epoch-1]\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b0684-fe3b-4928-88fd-5070db35aacc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training EMO-2M ====================\n",
      "[Missing] head.weight, head.bias...\n",
      "Reinitialized: head\n",
      "\n",
      "Epoch: 0\n",
      "Train Loss: 1.152 | OA: 49.8% | P: 34.5 | Se: 32.8 | Sp: 85.2 | F1: 32.5 | Kappa: 24.8 | T(avg): 2.472 | lambda(avg): 0.071\n",
      "Val  Loss: 0.914 | OA: 54.8% | P: 30.0 | Se: 33.8 | Sp: 86.0 | F1: 31.2 | AUC: 81.5 | Kappa: 29.9 | T(avg): 2.473 | lambda(avg): 0.125\n",
      "Epoch 1/10 Time: 2.1s\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 0.841 | OA: 60.6% | P: 51.9 | Se: 44.7 | Sp: 88.7 | F1: 44.7 | Kappa: 42.3 | T(avg): 2.472 | lambda(avg): 0.142\n",
      "Val  Loss: 0.809 | OA: 61.0% | P: 49.9 | Se: 47.3 | Sp: 88.8 | F1: 46.7 | AUC: 85.3 | Kappa: 43.2 | T(avg): 2.472 | lambda(avg): 0.155\n",
      "Epoch 2/10 Time: 2.1s\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 0.726 | OA: 64.0% | P: 57.2 | Se: 51.0 | Sp: 89.6 | F1: 52.4 | Kappa: 47.6 | T(avg): 2.472 | lambda(avg): 0.169\n",
      "Val  Loss: 0.698 | OA: 64.2% | P: 45.8 | Se: 46.4 | Sp: 89.7 | F1: 43.4 | AUC: 87.6 | Kappa: 47.7 | T(avg): 2.472 | lambda(avg): 0.185\n",
      "Epoch 3/10 Time: 2.1s\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 0.672 | OA: 65.6% | P: 57.5 | Se: 53.6 | Sp: 90.2 | F1: 55.0 | Kappa: 50.2 | T(avg): 2.472 | lambda(avg): 0.196\n",
      "Val  Loss: 0.689 | OA: 62.7% | P: 50.7 | Se: 47.3 | Sp: 88.7 | F1: 46.1 | AUC: 87.9 | Kappa: 43.9 | T(avg): 2.472 | lambda(avg): 0.207\n",
      "Epoch 4/10 Time: 1.9s\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 0.598 | OA: 67.8% | P: 63.0 | Se: 57.3 | Sp: 90.9 | F1: 58.9 | Kappa: 53.6 | T(avg): 2.472 | lambda(avg): 0.217\n",
      "Val  Loss: 0.634 | OA: 68.2% | P: 62.4 | Se: 54.5 | Sp: 90.9 | F1: 55.3 | AUC: 88.0 | Kappa: 53.9 | T(avg): 2.472 | lambda(avg): 0.225\n",
      "Epoch 5/10 Time: 2.0s\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 0.535 | OA: 70.2% | P: 65.5 | Se: 61.4 | Sp: 91.6 | F1: 62.9 | Kappa: 57.4 | T(avg): 2.472 | lambda(avg): 0.232\n",
      "Val  Loss: 0.650 | OA: 63.5% | P: 53.8 | Se: 53.4 | Sp: 89.8 | F1: 52.8 | AUC: 87.9 | Kappa: 47.6 | T(avg): 2.471 | lambda(avg): 0.240\n",
      "Epoch 6/10 Time: 1.9s\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 0.487 | OA: 75.4% | P: 72.1 | Se: 68.1 | Sp: 93.1 | F1: 69.7 | Kappa: 65.0 | T(avg): 2.471 | lambda(avg): 0.246\n",
      "Val  Loss: 0.618 | OA: 67.2% | P: 57.9 | Se: 55.6 | Sp: 91.0 | F1: 55.8 | AUC: 88.1 | Kappa: 53.2 | T(avg): 2.471 | lambda(avg): 0.252\n",
      "Epoch 7/10 Time: 2.0s\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 0.443 | OA: 75.2% | P: 72.7 | Se: 68.5 | Sp: 92.8 | F1: 70.1 | Kappa: 64.3 | T(avg): 2.471 | lambda(avg): 0.259\n",
      "Val  Loss: 0.758 | OA: 60.2% | P: 52.9 | Se: 48.0 | Sp: 89.0 | F1: 46.1 | AUC: 86.6 | Kappa: 43.4 | T(avg): 2.470 | lambda(avg): 0.264\n",
      "Epoch 8/10 Time: 1.9s\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 0.393 | OA: 78.1% | P: 75.1 | Se: 71.5 | Sp: 93.8 | F1: 72.9 | Kappa: 68.8 | T(avg): 2.470 | lambda(avg): 0.271\n",
      "Val  Loss: 0.723 | OA: 59.5% | P: 51.0 | Se: 49.7 | Sp: 88.8 | F1: 49.8 | AUC: 85.1 | Kappa: 42.1 | T(avg): 2.469 | lambda(avg): 0.276\n",
      "Epoch 9/10 Time: 1.8s\n",
      "\n",
      "Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    for config in model_configs:\n",
    "        model_name, model_builder = config\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\n{'='*20} Training {model_name} {'='*20}\")\n",
    "        model = model_builder().to(device)\n",
    "        train_and_save_model(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                train_loader=train_loader,\n",
    "                test_loader=test_loader,\n",
    "                num_classes = NUM_CLASSES,\n",
    "                num_epochs=10,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3d129-84bd-4aaf-951f-1f1a927fec4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
